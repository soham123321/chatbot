{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2ea68b-7d37-463c-b347-c7082ac68e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minio\n",
      "  Downloading minio-7.2.7-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 716 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from minio) (2021.5.30)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.9/site-packages (from minio) (20.1.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from minio) (3.10.0.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.9/site-packages (from minio) (1.26.6)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from argon2-cffi->minio) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->minio) (2.20)\n",
      "Installing collected packages: pycryptodome, minio\n",
      "Successfully installed minio-7.2.7 pycryptodome-3.20.0\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.34.153-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.35.0,>=1.34.153\n",
      "  Downloading botocore-1.34.153-py3-none-any.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 14.8 MB/s eta 0:00:01    |███████▏                        | 2.8 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 626 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.153->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.153->boto3) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.153->boto3) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.34.153 botocore-1.34.153 jmespath-1.0.1 s3transfer-0.10.2\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.1.2-bin-hadoop3.2/python (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (2.26.0)\n",
      "Collecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests) (2.0.0)\n",
      "Installing collected packages: py4j\n",
      "Successfully installed py4j-0.10.9\n",
      "Requirement already satisfied: pyspark in /usr/local/spark-3.1.2-bin-hadoop3.2/python (3.1.2)\n",
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[K     |████████████████████████████████| 246 kB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.9/site-packages (from pyspark) (0.10.9)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: confluent-kafka\n",
      "Successfully installed confluent-kafka-2.5.0\n",
      "Collecting newsdataapi\n",
      "  Downloading newsdataapi-0.1.18-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.9/site-packages (from newsdataapi) (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0->newsdataapi) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0->newsdataapi) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0->newsdataapi) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0->newsdataapi) (2.0.0)\n",
      "Installing collected packages: newsdataapi\n",
      "Successfully installed newsdataapi-0.1.18\n"
     ]
    }
   ],
   "source": [
    "!pip install minio\n",
    "!pip install boto3\n",
    "!pip install pyspark requests\n",
    "!pip install pyspark kafka-python\n",
    "!pip install confluent_kafka\n",
    "!pip install newsdataapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdafa7e-81d6-4525-b2b9-86484736af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import threading\n",
    "\n",
    "# MinIO and Kafka configuration\n",
    "minio_url = \"minio:9000\"\n",
    "access_key = \"access_key\"\n",
    "secret_key = \"secret_key\"\n",
    "kafka_bootstrap_servers = \"broker:29092\"\n",
    "kafka_topic = \"news\"\n",
    "news_api_key = \"api_key\"\n",
    "# news_api_key = \"api_key\"\n",
    "\n",
    "# Initialize Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=kafka_bootstrap_servers,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Initialize Kafka Admin Client and create topic if it doesn't exist\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=kafka_bootstrap_servers)\n",
    "topic = NewTopic(name=kafka_topic, num_partitions=1, replication_factor=1)\n",
    "try:\n",
    "    admin_client.create_topics(new_topics=[topic], validate_only=False)\n",
    "    print(f\"Topic '{kafka_topic}' created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Topic '{kafka_topic}' may already exist or an error occurred: {e}\")\n",
    "\n",
    "# Initialize MinIO Client\n",
    "minio_client = Minio(\n",
    "    minio_url,\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "def fetch_news(api_key, producer, kafka_topic, news_topic):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={news_topic}&apiKey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        articles = response.json().get('articles', [])\n",
    "        if articles:\n",
    "            for article in articles:\n",
    "                producer.send(kafka_topic, value=article)\n",
    "            producer.flush()\n",
    "            print(f\"Sent {len(articles)} articles to Kafka topic '{kafka_topic}'\")\n",
    "        else:\n",
    "            print(f\"No articles found for topic: {news_topic}\")\n",
    "    else:\n",
    "        print(f\"Error fetching news: {response.status_code}\")\n",
    "\n",
    "def upload_to_minio(article, news_topic):\n",
    "    bucket_name = re.sub(r'[^\\w]', '_', news_topic)  \n",
    "\n",
    "    # Parse the published date\n",
    "    published_date = datetime.fromisoformat(article['publishedAt'][:-1])\n",
    "    folder_name = published_date.strftime(\"%Y%m%d\") \n",
    "\n",
    "    try:\n",
    "        if not minio_client.bucket_exists(bucket_name):\n",
    "            minio_client.make_bucket(bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' created.\")\n",
    "    except S3Error as e:\n",
    "        print(f\"Error creating bucket: {e}\")\n",
    "    \n",
    "    title = re.sub(r'[^\\w\\s]', '_', article['title']).replace(' ', '_')\n",
    "    object_name = f\"{folder_name}/{title}-{published_date.strftime('%H%M%S')}.json\"  \n",
    "\n",
    "    article_json = json.dumps(article)\n",
    "    article_bytes = article_json.encode('utf-8')\n",
    "    article_stream = BytesIO(article_bytes)\n",
    "\n",
    "    minio_client.put_object(\n",
    "        bucket_name,\n",
    "        object_name,\n",
    "        article_stream,\n",
    "        length=len(article_bytes),\n",
    "        content_type='application/json'\n",
    "    )\n",
    "    \n",
    "    print(f\"Uploaded article to {bucket_name}/{object_name}\")\n",
    "\n",
    "def produce_news():\n",
    "    while True:\n",
    "        fetch_news(news_api_key, producer, kafka_topic, \"paris\")\n",
    "        time.sleep(300)\n",
    "\n",
    "def consume_news():\n",
    "    consumer = KafkaConsumer(\n",
    "        kafka_topic,\n",
    "        bootstrap_servers=kafka_bootstrap_servers,\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=True,\n",
    "        group_id='olympics'\n",
    "    )\n",
    "    \n",
    "    print(f\"Listening for messages on topic '{kafka_topic}'...\")\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            article = message.value\n",
    "            upload_to_minio(article, \"paris\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping consumer...\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start producer and consumer in separate threads\n",
    "    producer_thread = threading.Thread(target=produce_news)\n",
    "    consumer_thread = threading.Thread(target=consume_news)\n",
    "\n",
    "    producer_thread.start()\n",
    "    consumer_thread.start()\n",
    "\n",
    "    producer_thread.join()\n",
    "    consumer_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c10c1-8257-4440-b3e7-c96e9b9e1cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
